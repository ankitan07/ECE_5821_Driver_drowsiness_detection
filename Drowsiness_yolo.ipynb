{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imutils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python drowniness_yawn.py --webcam webcam_index\n",
    "\n",
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "from threading import Thread\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to give alarm message\n",
    "\n",
    "def alarm(msg):\n",
    "    global alarm_status\n",
    "    global alarm_status2\n",
    "    global saying\n",
    "\n",
    "    while alarm_status:\n",
    "        print('call')\n",
    "        s = 'espeak \"'+msg+'\"'\n",
    "        os.system(s)\n",
    "\n",
    "    if alarm_status2:\n",
    "        print('call')\n",
    "        saying = True\n",
    "        s = 'espeak \"' + msg + '\"'\n",
    "        os.system(s)\n",
    "        saying = False\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "\n",
    "    return ear\n",
    "\n",
    "def final_ear(shape):\n",
    "    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "\n",
    "    leftEye = shape[lStart:lEnd]\n",
    "    rightEye = shape[rStart:rEnd]\n",
    "\n",
    "    leftEAR = eye_aspect_ratio(leftEye)\n",
    "    rightEAR = eye_aspect_ratio(rightEye)\n",
    "\n",
    "    ear = (leftEAR + rightEAR) / 2.0\n",
    "    return (ear, leftEye, rightEye)\n",
    "\n",
    "def lip_distance(shape):\n",
    "    top_lip = shape[50:53]\n",
    "    top_lip = np.concatenate((top_lip, shape[61:64]))\n",
    "\n",
    "    low_lip = shape[56:59]\n",
    "    low_lip = np.concatenate((low_lip, shape[65:68]))\n",
    "\n",
    "    top_mean = np.mean(top_lip, axis=0)\n",
    "    low_mean = np.mean(low_lip, axis=0)\n",
    "\n",
    "    distance = abs(top_mean[1] - low_mean[1])\n",
    "    return distance\n",
    "\n",
    "\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-w\", \"--webcam\", type=int, default=0,\n",
    "                help=\"index of webcam on system\")   #if no of webcame is one than value is 0\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "EYE_AR_THRESH = 0.3\n",
    "EYE_AR_CONSEC_FRAMES = 30   #how many frmae eye closed\n",
    "YAWN_THRESH = 20\n",
    "alarm_status = False\n",
    "alarm_status2 = False\n",
    "saying = False\n",
    "COUNTER = 0\n",
    "\n",
    "print(\"-> Loading the predictor and detector...\")\n",
    "#detector = dlib.get_frontal_face_detector()\n",
    "detector = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")    #Faster but less accurate and detects face\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')   #gives landmark on face\n",
    "\n",
    "\n",
    "print(\"-> Starting Video Stream\")\n",
    "vs = VideoStream(src=args[\"webcam\"]).start()\n",
    "#vs= VideoStream(usePiCamera=True).start()       //For Raspberry Pi\n",
    "time.sleep(1.0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width=450)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #rects = detector(gray, 0)\n",
    "    rects = detector.detectMultiScale(gray, scaleFactor=1.1,\n",
    "\t\tminNeighbors=5, minSize=(30, 30),\n",
    "\t\tflags=cv2.CASCADE_SCALE_IMAGE)   #to detect objects in an image using the Haar Cascade Classifier\n",
    "\n",
    "    #for rect in rects:\n",
    "    for (x, y, w, h) in rects:\n",
    "        rect = dlib.rectangle(int(x), int(y), int(x + w),int(y + h))\n",
    "\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        eye = final_ear(shape)\n",
    "        ear = eye[0]\n",
    "        leftEye = eye [1]\n",
    "        rightEye = eye[2]\n",
    "\n",
    "        distance = lip_distance(shape)\n",
    "\n",
    "        leftEyeHull = cv2.convexHull(leftEye)\n",
    "        rightEyeHull = cv2.convexHull(rightEye)\n",
    "        cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
    "        cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "\n",
    "        lip = shape[48:60]\n",
    "        cv2.drawContours(frame, [lip], -1, (0, 255, 0), 1)\n",
    "\n",
    "        if ear < EYE_AR_THRESH:\n",
    "            COUNTER += 1\n",
    "\n",
    "            if COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
    "                if alarm_status == False:\n",
    "                    alarm_status = True\n",
    "                    t = Thread(target=alarm, args=('wake up sir',))\n",
    "                    t.deamon = True\n",
    "                    t.start()\n",
    "\n",
    "                cv2.putText(frame, \"DROWSINESS ALERT!\", (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        else:\n",
    "            COUNTER = 0\n",
    "            alarm_status = False\n",
    "\n",
    "        if (distance > YAWN_THRESH):\n",
    "                cv2.putText(frame, \"Yawn Alert\", (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                if alarm_status2 == False and saying == False:\n",
    "                    alarm_status2 = True\n",
    "                    t = Thread(target=alarm, args=('take some fresh air sir',))\n",
    "                    t.deamon = True\n",
    "                    t.start()\n",
    "        else:\n",
    "            alarm_status2 = False\n",
    "\n",
    "        cv2.putText(frame, \"EAR: {:.2f}\".format(ear), (300, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        cv2.putText(frame, \"YAWN: {:.2f}\".format(distance), (300, 60),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install and Import Dependencies\n",
    "# conda create --name yolo \n",
    "# conda activate yolo\n",
    "\n",
    "#conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
    "\n",
    "# pip3 install torch torchvision torchaudio\n",
    "# pip3 install --upgrade pip\n",
    "# pip3 install torch torchvision torchaudio\n",
    "# conda install python=3.9\n",
    "# python --version\n",
    "#touch requirements.txt #to create a fie\n",
    "# !pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
    "# !git clone https://github.com/ultralytics/yolov5\n",
    "# !cd yolov5 & pip install -r requirements.txt    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=train model='//yolov5s.pt' imgsz=640 data='/content/drive/MyDrive/Data_Final_Exp/Train Data/pt.yaml'  epochs=7 batch=1 name=yolovx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Make Detections with Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Cars_in_traffic_in_Auckland%2C_New_Zealand_-_copyright-free_photo_released_to_public_domain.jpg/800px-Cars_in_traffic_in_Auckland%2C_New_Zealand_-_copyright-free_photo_released_to_public_domain.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(img)\n",
    "results.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "plt.imshow(np.squeeze(results.render()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Real Time Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) #0 as only 1 cam\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Make detections \n",
    "    results = model(frame)\n",
    "    \n",
    "    cv2.imshow('YOLO', np.squeeze(results.render()))\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid   # Unique identifier\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = os.path.join('data', 'images') #/data/images\n",
    "labels = ['awake', 'drowsy']  #yawing \n",
    "number_imgs = 5\n",
    "# collect iamge from video \n",
    "cap = cv2.VideoCapture(0) #camera index 0\n",
    "# Loop through labels\n",
    "for label in labels:\n",
    "    print('Collecting images for {}'.format(label))\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Loop through image range\n",
    "    for img_num in range(number_imgs):\n",
    "        print('Collecting images for {}, image number {}'.format(label, img_num))\n",
    "        \n",
    "        # Webcam feed\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Naming out image path\n",
    "        imgname = os.path.join(IMAGES_PATH, label+'.'+str(uuid.uuid1())+'.jpg')\n",
    "        \n",
    "        # Writes out image to file \n",
    "        cv2.imwrite(imgname, frame)\n",
    "        \n",
    "        # Render to the screen\n",
    "        cv2.imshow('Image Collection', frame)\n",
    "        \n",
    "        # 2 second delay between captures\n",
    "        time.sleep(2)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.path.join(IMAGES_PATH, labels[0]+'.'+str(uuid.uuid1())+'.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "    print('Collecting images for {}'.format(label))\n",
    "    for img_num in range(number_imgs):\n",
    "        print('Collecting images for {}, image number {}'.format(label, img_num))\n",
    "        print(os.path.join(IMAGES_PATH, label+'.'+str(uuid.uuid1())+'.jpg'))\n",
    "        imgname = os.path.join(IMAGES_PATH, label+'.'+str(uuid.uuid1())+'.jpg')\n",
    "        print(imgname)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/tzutalin/labelImg  #for labelling\n",
    "\n",
    "#instlaiing dependecy for labelling\n",
    "#for mac\n",
    "!pip3 install pyqt5 lxml\n",
    "!cd labelImg && pyrcc5 -o libs/resources.py resources.qrc\n",
    "python3 labelImg.py\n",
    "\n",
    "\n",
    "\n",
    "#for windows\n",
    "!pip install pyqt5 lxml --upgrade\n",
    "!cd labelImg && pyrcc5 -o libs/resources.py resources.qrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#batch sixe , image six of 320 , data.yml file, what model we want to train, it download from repo\n",
    "# to create dataset yml go to training in yolo5 in readme in the yolo5 repo, they have mnetion what to include in yaml file\n",
    "#!cd yolov5 && python train.py --img 320 --batch 16 --epochs 100 --data dataset.yaml --weights yolov5s.pt --workers 2\n",
    "\n",
    "python train.py --img 320 --batch 16 --epochs 100 --data dataset.yaml --weights yolov5s.pt --workers 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Load Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the custom model that has the weight as got in the above training\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5/runs/train/exp2/weights/last.pt', force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = os.path.join('data', 'images', 'drowsy.91e39a0c-384d-11ee-ab06-acde48001122.jpg')\n",
    "#drowsy.91e39a0c-384d-11ee-ab06-acde48001122.jpg\n",
    "results = model(img)\n",
    "results.print()\n",
    "%matplotlib inline \n",
    "plt.imshow(np.squeeze(results.render()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Make detections \n",
    "    results = model(frame)\n",
    "    \n",
    "    cv2.imshow('YOLO', np.squeeze(results.render()))\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new try \n",
    "https://www.youtube.com/watch?v=ksi42rwGyas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install dlib\n",
    "import dlib  # for landmark detection\n",
    "!git clone https://github.com/nicolasmetallo/eameo-faceswap-generator\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
